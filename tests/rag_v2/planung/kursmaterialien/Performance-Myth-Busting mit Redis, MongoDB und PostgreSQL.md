Hintergrund
Ihr habt in der LaborÃ¼bung die Schreibleistung von drei Systemen gemessen (jeweils 10 000 Writes):

Redis (naiv vs. Pipeline)

MongoDB (naiv vs. insert_many)

PostgreSQL (naiv mit 10 000 Commits vs. eine Transaktion)

Die Ergebnisse zeigten:

Bulk/Batching/Pipelining verÃ¤ndert die Performance extrem

Alle â€langsamenâ€œ Resultate kamen von Client-Overhead, nicht von der Engine

MongoDB war im â€Bulk Insertâ€œ-Szenario am schnellsten

Redis war ohne Pipeline deutlich langsamer als erwartet

PostgreSQL leidet massiv unter 10 000 einzelnen Commits

In dieser Reflexion geht es darum, was diese Resultate tatsÃ¤chlich bedeuten â€“ und was nicht.

ğŸŸ© Aufgabe 1 â€” Interpretation der Laborergebnisse (kritisch & prÃ¤zise)
Beantwortet die folgenden Fragen als Team (stichwortartig genÃ¼gt):

1.1 Wo lag in euren Messungen der tatsÃ¤chliche Bottleneck?
War es CPU? Die Datenbank? Das Netzwerk? Python?

Welche Hinweise aus euren Zahlen belegen das?

1.2 Warum war Redis â€naivâ€œ (ohne Pipeline) nicht besonders schnell?
ErklÃ¤rt:

was ein Roundtrip ist

warum Redis bei Einzel-SETs unterperformt

warum Redis mit Pipeline plÃ¶tzlich 10â€“15Ã— schneller wurde

1.3 Warum war MongoDB mit insert_many() so extrem schnell?
Diskutiert:

was ein Bulk-Insert intern bedeutet

warum dieses Szenario ein perfekter Spezialfall fÃ¼r Mongo ist

warum das nicht bedeutet, dass MongoDB generell schneller ist

1.4 Warum braucht PostgreSQL in einer einzigen Transaktion dramatisch weniger Zeit?
ErklÃ¤rt:

was ein Commit in PostgreSQL wirklich tut (WAL, fsync, Locks)

warum 10 000 einzelne Commits 10 000Ã— teuer sind

warum eine Transaktion einen vÃ¶llig anderen Kostentreiber hat

1.5 Zieht ein erstes Zwischenfazit:
Formuliert 3 SÃ¤tze zu:

â€Was sagt unser Experiment wirklich aus â€“ und was sagt es NICHT aus?â€œ

ğŸŸ§ Aufgabe 2 â€” Warum ist dieser Benchmark im echten Leben unrealistisch?
Diskutiert gemeinsam:

In wie vielen realen Systemen schreibt ihr 10 000 unabhÃ¤ngige einzelne Werte ohne Updates weg?

Welche Praxis-Szenarien sind viel typischer?
(z. B. Session-State, Produktdaten, Rechnungen, IoT-Sensoren, Cache-Kaltstarts, Benutzerprofileâ€¦)

Formuliert 3 Beispiele aus realen Anwendungen, die dieses Labor-Beispiel nicht abbildet.

ğŸŸ¨ Aufgabe 3 â€” Workload-orientiertes Denken: Welche Fragen fehlen noch?
Euer bisheriger Test betrachtet nur einen einzigen Aspekt: groÃŸe Menge Writes.

Jetzt sollt ihr Ã¼berlegen, welche anderen Dimensionen man testen mÃ¼sste, um wirklich sesuatu sinnvolle Architekturentscheidungen treffen zu kÃ¶nnen.

Erstellt eine Liste mit mindestens 5 weiteren Performance-Fragen, z. B.:

Wie schnell sind Reads (Redis GET vs. Mongo find vs. PG SELECT)?

Wie verhalten sich die Systeme bei Mixed Workloads (50% Read / 50% Write)?

Was passiert bei concurrent Writes?

Was passiert bei Updates statt Inserts?

Wie teuer ist ein Update auf ein GroÃŸes Dokument?

Wie wirkt sich TTL (Redis EXPIRE) auf die Last aus?

Wie beeinflusst Indexierung die Write-Performance?

Wie sieht die Latenz (P95/P99) bei hoher Last aus?

Wie skaliert das System, wenn viele Clients gleichzeitig schreiben?

WÃ¤hlt aus eurer Liste die 3 wichtigsten fÃ¼r die Praxis aus und begrÃ¼ndet eure Auswahl.

ğŸŸ« Aufgabe 4 â€” Design Thinking: neue Labor-Experimente vorschlagen
Basierend auf euren Erkenntnissen formuliert ihr mindestens zwei neue Labor-Experimente, die:

technisch interessant sind

echte Systemvergleiche ermÃ¶glichen

typische Backend-Probleme widerspiegeln

Beispiele (zur Inspiration, nicht vorgeben!):

TTL-Test: 100 000 Keys mit EXPIRE erzeugen â†’ wie teuer ist Auto-Cleanup?

Update-Test: 10 000 Updates auf JSON-Dokumente â†’ Verhalten bei Partial-Updates

Cache-Hit-Test: 99% Reads, 1% Writes â†’ wer gewinnt in Latenz?

Bulk-Update-Test: 10 000 Felder eines Objekts Ã¤ndern (Mongo vs. PG vs. Redis)

Mixed Workload: 70% GET, 30% SET/INSERT

IoT-Szenario: 1 000 Sensoren schreiben Werte â†’ wie stabil sind die Systeme?

Formuliert pro vorgeschlagenem Experiment:

Ziel

Hypothese

warum es realistisch ist

welche Metriken man messen mÃ¼sste

ğŸŸ¥ Abgabe
Ein PDF oder Markdown-Dokument pro Team mit:

Ergebnissen zu Aufgabe 1

Diskussion zu Aufgabe 2

Liste + Priorisierung zu Aufgabe 3

2â€“3 ausgearbeiteten VorschlÃ¤gen fÃ¼r Aufgabe 4