Chunking-Strategien & Retrieval Tuning (3â€“5 Minuten)
Titel: â€Warum euer RAG-System steht und fÃ¤llt mit guten Chunksâ€œ
1. Einstieg
Viele Leute denken:

â€Chunking ist trivial â€” einfach alle X WÃ¶rter schneiden.â€œ

Das ist das klassische AnfÃ¤ngerloch.
Ãœber 50 % der Retrieval-Fehler entstehen durch schlechtes Chunking.

2. Warum Chunking Ã¼berhaupt notwendig ist
LLMs kÃ¶nnen nur begrenzte Kontextfenster lesen.
Daher mÃ¼ssen groÃŸe Dokumente in kleine Teile (Chunks) zerlegt werden:

200â€“500 Tokens pro Chunk

mit Overlap von 20â€“30 %

Das Ziel: Jeder Chunk enthÃ¤lt eine kohÃ¤rente semantische Einheit.

3. Schlechte Chunking-Strategien (Beispiele)
âŒ â€Snippet every 500 charactersâ€œ
â†’ bricht SÃ¤tze, Tabellen, Abschnitte auf
â†’ Embeddings werden inhaltlich wertlos

âŒ â€Einfach immer 200 Tokensâ€œ
â†’ trennt Rezepte, Tabellen, Definitionen mitten durch
â†’ KÃ¼nstliche BrÃ¼che, semantisch falsch

4. Gute Chunking-Strategien
A) Absatz-basiertes Chunking
Chunks entsprechen Textabschnitten.
Semantisch sauber, Retrieval hat hohe Precision.

B) Heading-aware Chunking
Ãœberschriften definieren Logik:

Installation
Fehlerbehebung
Garantiebedingungen
Chunks enthalten je einen vollstÃ¤ndigen Abschnitt.

C) Semantisches Chunking
Z. B. mit Transformers / Sentence Embedding-Scores.
Wird in der Industrie â†’ Standard.

5. Chunk Overlap (warum es wichtig ist)
Overlap von 15â€“30 % lÃ¶st:

SatzabbrÃ¼che

verlorene KontextwÃ¶rter

â€vergesseneâ€œ Schlussfolgerungen

Ohne Overlap verpasst die ANN-Suche oft die relevanten Teile â†’ schlechte Recall@K.

6. Retrieval Tuning â€” was ANN wirklich braucht
Parameter, die entscheiden:
K (Top-K)

ef_search (HNSW)

IndexgrÃ¶ÃŸe (HNSW M)

Normalisierung (z. B. fÃ¼r Cosine)

Praktische Faustregeln:

K = 8â€“12 â†’ meistens ideal

ef_search = 20â€“50 (QualitÃ¤t vs. Latenz)

Keine Cosine-Suche ohne Normalisierung

Keine L2-Suche mit unnormalisierten Embeddings

7. Kleines Experiment fÃ¼r heute (Labor)
Testet:

Chunk-GrÃ¶ÃŸe 150 vs 300 vs 500 Tokens

Overlap 0 % vs 20 %

K = 5 vs K = 10 vs K = 20

Mit/ohne Ãœberschriften-Basiertes Chunking

â†’ Schaut euch an:

Precision (wie viele Treffer sind wirklich relevant?)

Recall (wie viele relevante Chunks fehlen?)

Latenz

Erwartetes Ergebnis:
ChunkgrÃ¶ÃŸe & Overlap erzeugen deutlich mehr Wirkung als Vektor-Modelle oder Indexparameter.


â€Chunks allein sind wertlos.â€œ
Warum jeder Chunk eine sichere RÃ¼ckverbindung braucht**

Kurz, klar, wichtig:

Ein Chunk ist nur ein StÃ¼ck Text.
Ohne Kontext ist er bedeutungslos.
Eure wichtigste Aufgabe ist nicht das Chunking selbst,
sondern die Modellierung der Beziehungen.

Was jeder Chunk in einem RAG-System immer haben muss:

chunk_id â€“ PrimÃ¤rschlÃ¼ssel

text â€“ der eigentliche Ausschnitt

embedding â€“ Vektor

doc_id â€“ Verweis auf das Ursprungsdokument

abschnitt_id / section_title â€“ semantischer Kontext

produkt â†’ version â†’ dokumenttyp â€“ Metadaten

position (â€chunk_numberâ€œ) im Dokument â€“ Reihenfolge

ğŸ‘‰ Ohne diese Beziehungen ist ein RAG-System nicht baubar.

Fazit:
Vektorsuche liefert nur den SchlÃ¼ssel zu den wichtigen Chunks.
Der Prompt braucht:

den Text

den Namen des Dokuments

die Version

evtl. mehrere aufeinanderfolgende Chunks

und manchmal die Ãœberschrift oder umliegenden Kontext

Die drei realistischsten Speicherstrategien (mit Entscheidungskriterien)
Strategie A: Alles-in-Postgres
Tabelle: chunks

| chunk_id | doc_id | text | embedding | metadata_jsonb | chunk_num |

Vorteile:

einfaches JOIN

starke Konsistenz

sauberer Filter

weniger Moving Parts

ideal zum Lernen

Nachteile:

nicht super performant fÃ¼r Millionen von Chunks

BLOB-Handling mÃ¤ÃŸig

Wann gut?
ğŸ‘‰ RAG mit < 500.000 Chunks
ğŸ‘‰ Uni-Projekte
ğŸ‘‰ Systeme, die starke Filter brauchen

Strategie B: pgvector fÃ¼r Embeddings + MongoDB fÃ¼r Text & Metadaten
Vorteile:

Mongo = perfekter Document Store fÃ¼r semistrukturierte Infos

pgvector = starker ANN-Index

extrem groÃŸe Datenmengen skalierbar

saubere Trennung: Text hier, Vektor dort

Nachteile:

Cross-DB-Queries brauchen Applikationslogik

etwas komplexer

Wann gut?
ğŸ‘‰ Reale Enterprise-Workloads
ğŸ‘‰ Viele Dokumentarten (PDF, HTML, Logs, Tabellen)

Strategie C: Vectorindex + Filestore + Metadata-DB
z. B.:

Vektoren in pgvector

Text in S3 / MinIO

Metadaten in Postgres/Mongo

doc_id referenziert die Files

Vorteile:

Skalierbar

Billiger Storage fÃ¼r groÃŸe PDFs

Ideal fÃ¼r mehrsprachige & groÃŸe Dokus

Nachteile:

Aufwendiger zu implementieren

Wann gut?
ğŸ‘‰ Massive Dokumente (>100 MB)
ğŸ‘‰ GroÃŸe Enterprises (ServiceNow, Salesforce etc.)

Wo Chunks schiefgehen (und was das fÃ¼r eure Speicherung bedeutet)


1. â€Ich speichere nur die Embeddings.â€œ
â†’ Ergebnis: Du bekommst chunk_ids, aber du weiÃŸt nicht:

aus welchem Dokument

in welchem Kontext

welche Version

welcher Abschnitt
â†’ vÃ¶llig unbrauchbar fÃ¼r Prompts.

2. â€Ich speichere Text, aber nicht chunk_num.â€œ
â†’ Du kannst keine zusammenhÃ¤ngenden Chunks laden.

3. â€Ich speichere Text ohne Ãœberschrift / section_title.â€œ
â†’ LLM hat keinen Kontext â†’ hÃ¶here Halluzinationsrate.

4. â€Ich speichere Dokumentversionen nicht.â€œ
â†’ Du mixst v1.0 und v3.2 und erzeugst Falschaussagen.

5. â€Ich speichere Metadaten nicht sauber.â€œ
â†’ Filtering klappt nicht â†’ RAG wird unprÃ¤zise.

â€Baut drei Varianten einer Chunk-Datenbank und vergleicht die QualitÃ¤tâ€œ
Ziel:
wollen sehen dass
Speicherstrategie = RetrievalqualitÃ¤t beeinflusst.

ğŸŸ£ Schritt 1 â€” Drei Speichermodelle implementieren
A) Minimales Modell (schlecht)
Nur:

chunk_id

embedding

text

â†’ kein doc_id, keine Metadaten

B) Besseres Modell (gut)
chunk_id

embedding

text

doc_id

chunk_num

section_title

C) Best-Practice Modell (exzellent)
chunk_id

embedding

text

doc_id

chunk_num

section_title

product_family

version

visibility

document_type

created_at

Schritt 2 â€” Sucht dieselbe Frage in allen drei Modellen
Beispiel:

â€Warum blinkt die Kaffeemaschine gelb?â€œ

Messt:

Relevanz der Top-3 Chunks

VollstÃ¤ndigkeit der Information

KontextqualitÃ¤t

Prompt-LÃ¤nge

Zeit bis zur Finalantwort

Schritt 3 â€” Bewertet nach Punkten
Kriterium	Minimal	Gut	Exzellent
Relevanz	2/10	6/10	9/10
Kontext	1/10	6/10	10/10
LLM-QualitÃ¤t	3/10	7/10	10/10
Filtering	0/10	5/10	10/10
Geschwindigkeit	irrelevant	relevant	sehr gut
Schritt 4 â€” Diskussion
Frage:
â€Was hÃ¤ttet ihr beim Modell A niemals rekonstruieren kÃ¶nnen?â€œ

Erwartete Einsichten:

Dokumentkontext fehlt

Reihenfolge fehlt

Metadaten fehlen

Prompt wird schlecht

RAG hat â€falsche Erinnerungâ€œ

Fazit :
Chunking ist nicht das Schneiden von Text,
Chunking ist das Modellieren von Beziehungen zwischen Fragmenten und dem Gesamtdokument.

Ein guter Chunk ist nicht nur Text + Embedding,
sondern ein eingebetteter Datenknoten in einem sauberen Datenmodell.


Hinweis:
Metadaten beim INGEST (Dokument â†’ Metadaten)
Frage: â€Wie bekommt jedes Dokument seine Metadaten?â€œ

Antwort:
â†’ Deterministisch. Immer. Ohne LLM.

Warum?

Das Dokument gehÃ¶rt zu Produkt X (steht im Dateipfad).

Das Dokument ist Modell KA-22 (steht im Titel, SKU-Katalog).

Dokumenttyp ist Troubleshooting (kommt aus Ordnerstruktur).

Version kommt aus Datenbank oder YAML-Header.

Beispiele aus echten Systemen:

/products/coffee/ka-22/troubleshooting_v2024.pdf

â€Kaffeemaschine KA-22 Bedienungsanleitungâ€œ

Diese Infos sind objektiv bekannt, nicht zu erraten.

Deshalb:

â€Man nutzt NICHT das LLM, um zu erraten, ob ein Chunk zu Kaffeemaschine gehÃ¶rt.â€œ

Denn: Die Maschine â€weiÃŸâ€œ schon vorher, dass ein Chunk zur Kaffeemaschine gehÃ¶rt, weil das in den Metadaten des Dokuments steht.

2. Metadaten beim Retrieval: Filter auf die USER-FRAGE anwenden (Query â†’ Filter)
Frage: â€Wie finde ich heraus, welche Metadatenfilter auf die Frage des Nutzers angewendet werden sollen?â€œ

User fragt:

â€Warum blinkt meine Kaffeemaschine gelb?â€œ

Jetzt muss das System entscheiden:

Produktfamilie â†’ Kaffeemaschine

Dokumenttyp â†’ Troubleshooting

Sprache â†’ de

Rolle â†’ customer

Modell â†’ vielleicht KA-22 (wenn explizit genannt)

Diese Informationen stehen NICHT in den Dokumenten, sondern mÃ¼ssen aus der Userfrage extrahiert werden.

Und dafÃ¼r nutzt man i. d. R. ein LLM oder klassisches ML.

Zusammenfassung:
Dokumente â†’ Metadaten = deterministisch, kein LLM

User-Frage â†’ Filter = Klassifikation, oft mit LLM